# -*- coding: utf-8 -*-
"""music_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ckw1_0st9VIl_EkLe4etAigwx5Ts9IWS
"""

import os
from music21 import converter, instrument, note, chord

# !apt-get update -qq
# !apt-get install -qq musescore
# !pip install music21 torch torchvision tqdm

# !unzip -q /content/drive/MyDrive/maestro-v3.0.0-midi.zip -d /content/maestro_dataset

def get_notes_recursive(base_dir):
    from music21 import converter, instrument, note, chord
    import os

    notes = []
    for root, _, files in os.walk(base_dir):
        i = 0
        for file in files:
            if file.endswith(".midi") or file.endswith(".mid"):
                try:
                    midi = converter.parse(os.path.join(root, file))
                    parts = instrument.partitionByInstrument(midi)
                    elements = parts.parts[0].recurse() if parts else midi.flat.notes
                    for e in elements:
                        if isinstance(e, note.Note):
                            notes.append(str(e.pitch))
                        elif isinstance(e, chord.Chord):
                            notes.append('.'.join(str(n) for n in e.normalOrder))
                except Exception as e:
                    print(f"Error parsing {file}: {e}")
                    continue
            i += 1
            if i == 1:
              break
    return notes
notes = get_notes_recursive(r'/home/bandham/Documents/Deep Learning - Upskilling/RNN/maestro-v3.0.0-midi/maestro-v3.0.0')
print(f"Collected {len(notes)} notes/chords from all folders.")

import numpy as np

sequence_length = 100
pitch_names = sorted(set(notes))
note_to_idx = {n: i for i, n in enumerate(pitch_names)}
idx_to_note = {i: n for i, n in enumerate(pitch_names)}

network_input = []
network_output = []
n_vocab = len(pitch_names)

for i in range(len(notes) - sequence_length):
    inp = [note_to_idx[n] for n in notes[i : i + sequence_length]]
    out = note_to_idx[notes[i + sequence_length]]
    network_input.append(inp)
    network_output.append(out)

network_input = np.array(network_input)
network_output = np.array(network_output)
print(network_input.shape, network_output.shape)

import torch
from torch.utils.data import Dataset, DataLoader

class MusicDataset(Dataset):
    def __init__(self, inputs, outputs):
        self.inputs = torch.tensor(inputs, dtype=torch.long)
        self.outputs = torch.tensor(outputs, dtype=torch.long)
    def __len__(self):
        return len(self.inputs)
    def __getitem__(self, idx):
        return self.inputs[idx], self.outputs[idx]

dataset = MusicDataset(network_input, network_output)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

import torch.nn as nn

class MusicLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim=256, lstm_units=512, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, lstm_units, num_layers, batch_first=True)
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(lstm_units, vocab_size)

    def forward(self, x, hidden=None):
        x = self.embedding(x)
        out, hidden = self.lstm(x, hidden)
        out = self.dropout(out[:, -1, :])
        return self.fc(out), hidden

model = MusicLSTM(n_vocab)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

from tqdm import tqdm

epochs = 5
model.train()
for epoch in range(1, epochs + 1):
    total_loss = 0
    for x, y in tqdm(dataloader, desc=f"Epoch {epoch}"):
        # x, y = x.to('cuda'), y.to('cuda')
        optimizer.zero_grad()
        logits, _ = model(x)
        loss = criterion(logits, y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch} Loss: {total_loss / len(dataloader):.4f}")

def generate_notes(model, start_pattern, length=500):
    model.eval()
    pattern = start_pattern.copy()
    gen = []

    for _ in range(length):
        inp = torch.tensor([pattern[-sequence_length:]], dtype=torch.long)
        logits, _ = model(inp)
        idx = torch.argmax(logits, dim=-1).item()
        gen.append(idx)
        pattern.append(idx)
    return gen

# Pick a random seed from input patterns
import random
seed = random.choice(network_input.tolist())
gen_idxs = generate_notes(model, seed)
gen_notes = [idx_to_note[i] for i in gen_idxs]

from music21 import stream

def create_midi(prediction, output_file='gen_output.mid'):
    offset = 0
    out_notes = []
    for pattern in prediction:
        if '.' in pattern:
            parts = pattern.split('.')
            c = chord.Chord([int(p) for p in parts])
            c.offset = offset
            out_notes.append(c)
        else:
            n = note.Note(pattern)
            n.offset = offset
            out_notes.append(n)
        offset += 0.5
    midi_stream = stream.Stream(out_notes)
    midi_stream.write('midi', fp=output_file)

create_midi(gen_notes, 'generated.mid')




